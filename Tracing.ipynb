{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnxc4k3jT9v4JPWb+j7vok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubaahmedkhan/AgenticAI-Toolkit-/blob/main/Tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_f2bN_NVkgw",
        "outputId": "4581f51b-e863-436e-fe5e-020815886477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "cgUDBIzxWAbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, ModelSettings, function_tool\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "ucW3rKjUWG2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "# Check if the API key is present; if not, raise an error\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "model_settings = ModelSettings(\n",
        "    max_tokens=50,\n",
        "                                              # CHECK All PARAMETERS PICTURE IN MOBILE\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    model_settings=model_settings,\n",
        "    tracing_disabled=True\n",
        ")"
      ],
      "metadata": {
        "id": "CtvFu_U3WUb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"helpful assistant\",\n",
        "    instructions=\"You are a helpful assistant.\",\n",
        ")\n",
        "\n",
        "result = await Runner.run(agent, \"hello, how are you\", run_config=config)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk7YaPwKWpg3",
        "outputId": "a026d367-4331-4a9a-f267-f6c9ea3b45ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"helpful assistant\", ...)\n",
            "- Final output (str):\n",
            "    Hello! As an AI, I don't experience feelings in the same way humans do. But I'm functioning well and ready to assist you. How can I help you today?\n",
            "- 1 new item(s)\n",
            "- 1 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.last_agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUG1uTj8YIm6",
        "outputId": "d6b2ae63-e236-4d36-a362-a52df2524278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Agent(name='helpful assistant', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.output_guardrail_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToQZSTcIYlfq",
        "outputId": "b51e1080-9f75-4512-e3ea-fd7c0e9bdc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.new_items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0BOJuDbY1Zx",
        "outputId": "41c16342-7179-4d13-ebc7-0feee4ad1686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[MessageOutputItem(agent=Agent(name='helpful assistant', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Hello! How can I help you today?\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.raw_responses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z2I3m23ZC4Y",
        "outputId": "954fe2ac-4999-473f-a788-98825d099335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Hello! As an AI, I don't experience feelings in the same way humans do, but I am functioning well and ready to assist you. How can I help you today?\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=11, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=38, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=49), response_id=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **💡 Trace aur Span ka matlab kya hai?**\n",
        "Jab aap kisi AI workflow ko chalatay hain — jaise ke ek agent user se baat karta hai, tool call karta hai, LLM se jawab mangta hai — to poora process ko ek Trace kehtay hain.\n",
        "\n",
        "Aur Trace ke andar chhoti chhoti activities ko Spans kehtay hain.\n",
        "\n",
        "🟦 Trace – Pura Workflow ka Record\n",
        "Trace represent karta hai ek poora end-to-end kaam — jaise:\n",
        "\n",
        "\"Customer Support Agent ne user ka masla hal kia\"\n",
        "\n",
        "Trace ke Properties:\n",
        "workflow_name – Workflow ka naam, jaise \"Code generation\" ya \"Customer support\".\n",
        "\n",
        "trace_id – Unique ID hoti hai har trace ki. Format: trace_<32 letters/numbers>. (Agar na dein to system khud bana deta hai.)\n",
        "\n",
        "group_id – Optional hai. Agar aap multiple traces ko ek group mein rakhna chahtay hain (jaise ek hi user ka chat session), to yeh use hota hai.\n",
        "\n",
        "disabled – Agar True ho, to trace record nahi hoti.\n",
        "\n",
        "metadata – Aap kuch extra information bhi trace ke sath attach kar saktay hain (jaise user ID, country etc.)\n",
        "\n",
        "\n",
        "🟨 Span – Trace ke Andar Ek Chhota Hissa\n",
        "Har trace ke andar chhoti chhoti steps hoti hain, jinko Spans kehtay hain.\n",
        "\n",
        "Jaise:\n",
        "\n",
        "Agent call karna\n",
        "\n",
        "LLM se response lena\n",
        "\n",
        "Tool run karna\n",
        "\n",
        "Data store karna\n",
        "\n",
        "Span ke Properties:\n",
        "started_at & ended_at – Start aur end ka time.\n",
        "\n",
        "trace_id – Ye span kis trace se related hai.\n",
        "\n",
        "parent_id – Agar yeh span kisi aur span ke andar hua, to uska ID.\n",
        "\n",
        "span_data – Yeh batata hai span kis kaam ke liye tha:\n",
        "\n",
        "AgentSpanData – agar agent chal raha tha\n",
        "\n",
        "GenerationSpanData – agar LLM se jawab aaya tha\n",
        "\n",
        "ToolSpanData – agar koi tool run hua tha\n",
        "\n",
        "\n",
        "**✅ Default Tracing ka Matlab**\n",
        "Jab aap OpenAI Agent SDK ka use karte hain, to SDK khud ba khud (automatically) kuch important cheezon ka trace record karta hai — aapko manually kuch likhne ki zarurat nahi hoti.\n",
        "\n",
        "\n",
        "| Kaam                                               | Span Type              | Kya Trace Hota Hai?                    |\n",
        "| -------------------------------------------------- | ---------------------- | -------------------------------------- |\n",
        "| `Runner.run()` ya `run_sync()` ya `run_streamed()` | `trace()`              | Poora execution                        |\n",
        "| Agent ka chalna                                    | `agent_span()`         | Jab agent kaam kar raha ho             |\n",
        "| LLM ka jawab dena                                  | `generation_span()`    | LLM se prompt bhejna aur response lena |\n",
        "| Tool ka call hona                                  | `function_span()`      | Jab koi tool use ho                    |\n",
        "| Guardrail ka kaam                                  | `guardrail_span()`     | Agar guardrails lage ho                |\n",
        "| Handoff (ek agent se doosray ko kaam dena)         | `handoff_span()`       | Jab agent doosray ko handoff kare      |\n",
        "| Audio input (Speech to Text)                       | `transcription_span()` | User ki awaaz ko text mein badalna     |\n",
        "| Audio output (Text to Speech)                      | `speech_span()`        | Text ko awaaz mein badalna             |\n",
        "| Related audio spans grouping                       | `speech_group_span()`  | Sab audio spans ko group karna         |\n",
        "\n",
        "\n",
        "\n",
        "🏷️ **Default Trace Name**\n",
        "SDK trace ka default naam hota hai: \"Agent trace\"\n",
        "\n",
        "Lekin agar aap chahein to trace() ya RunConfig mein naam aur properties customize kar saktay hain.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T5RCUJJ4bABe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from agents import Runner,RunConfig\n",
        "\n",
        "# Custom RunConfig\n",
        "run_config = RunConfig(\n",
        "    trace_name=\"Customer Support Workflow\",  # custom trace name\n",
        "    trace_metadata={\"user_id\": \"user_123\", \"session\": \"chat_456\"},\n",
        "    trace_group_id=\"conversation_group_1\"\n",
        ")\n",
        "\n",
        "# Runner with the custom config\n",
        "result = await Runner.run(agent=agent, input=\"Hello!\", run_config=run_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "USMQIQOsegNE",
        "outputId": "1882e26e-490a-4294-cfb6-91a12cfcf4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "RunConfig.__init__() got an unexpected keyword argument 'trace_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-2023049607.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Custom RunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m run_config = RunConfig(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrace_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Customer Support Workflow\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# custom trace name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrace_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user_123\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"session\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"chat_456\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: RunConfig.__init__() got an unexpected keyword argument 'trace_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Higher level traces**\n",
        "Sometimes, you might want multiple calls to run() to be part of a single trace. You can do this by wrapping the entire code in a trace()."
      ],
      "metadata": {
        "id": "Ix7RL_3dhIwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, trace\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n",
        "\n",
        "    with trace(\"Joke workflow\"):\n",
        "        first_result = await Runner.run(agent, \"Tell me a joke\",run_config=config)\n",
        "        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\",run_config=config)\n",
        "        print(f\"Joke: {first_result.final_output}\")\n",
        "        print(f\"Rating: {second_result.final_output}\")\n",
        "\n",
        "# For running the async main\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85x4m5dugSMZ",
        "outputId": "6ecd5980-93e9-42bd-c631-489b52db960d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joke: Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "Rating: Okay, that's a solid classic! I'd give it a **7/10**. It's punny, easily understandable, and gets a chuckle. It's not going to win any comedy awards, but it's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trace Locally**"
      ],
      "metadata": {
        "id": "VTHc3IU04-t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import AsyncOpenAI\n",
        "from agents import Agent, Runner, trace, set_default_openai_api, set_default_openai_client, set_trace_processors\n",
        "from agents.tracing.processor_interface import TracingProcessor\n",
        "from pprint import pprint\n",
        "# Custom trace processor to collect trace data locally\n",
        "class LocalTraceProcessor(TracingProcessor):\n",
        "    def __init__(self):\n",
        "        self.traces = []\n",
        "        self.spans = []\n",
        "\n",
        "    def on_trace_start(self, trace):\n",
        "        self.traces.append(trace)\n",
        "        print(f\"Trace started: {trace.trace_id}\")\n",
        "\n",
        "    def on_trace_end(self, trace):\n",
        "        print(f\"Trace ended: {trace.export()}\")\n",
        "\n",
        "    def on_span_start(self, span):\n",
        "        self.spans.append(span)\n",
        "        print(f\"Span started: {span.span_id}\")\n",
        "        print(f\"Span details: \")\n",
        "        pprint(span.export())\n",
        "\n",
        "    def on_span_end(self, span):\n",
        "        print(f\"Span ended: {span.span_id}\")\n",
        "        print(f\"Span details:\")\n",
        "        pprint(span.export())\n",
        "\n",
        "    def force_flush(self):\n",
        "        print(\"Forcing flush of trace data\")\n",
        "\n",
        "    def shutdown(self):\n",
        "        print(\"=======Shutting down trace processor========\")\n",
        "        # Print all collected trace and span data\n",
        "        print(\"Collected Traces:\")\n",
        "        for trace in self.traces:\n",
        "            print(trace.export())\n",
        "        print(\"Collected Spans:\")\n",
        "        for span in self.spans:\n",
        "            print(span.export())\n",
        "\n",
        "BASE_URL = os.getenv(\"BASE_URL\") or \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\") or userdata.get(\"GEMINI_API_KEY\")\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\") or \"gemini-2.0-flash\"\n",
        "\n",
        "\n",
        "if not BASE_URL or not API_KEY or not MODEL_NAME:\n",
        "    raise ValueError(\"Please set BASE_URL, GEMINI_API_KEY, MODEL_NAME via env var or code.\")\n",
        "\n",
        "# Create OpenAI client\n",
        "client = AsyncOpenAI(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        ")\n",
        "\n",
        "# Configure the client\n",
        "set_default_openai_client(client=client, use_for_tracing=True)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "\n",
        "# Set up the custom trace processor\n",
        "local_processor = LocalTraceProcessor()\n",
        "set_trace_processors([local_processor])\n",
        "\n",
        "# Example function to run an agent and collect traces\n",
        "async def main():\n",
        "    agent = Agent(name=\"Example Agent\", instructions=\"Perform example tasks.\", model=MODEL_NAME)\n",
        "\n",
        "    with trace(\"Example workflow\"):\n",
        "        first_result = await Runner.run(agent, \"Start the task\")\n",
        "        second_result = await Runner.run(agent, f\"Rate this result: {first_result.final_output}\")\n",
        "        print(f\"Result: {first_result.final_output}\")\n",
        "        print(f\"Rating: {second_result.final_output}\")\n",
        "\n",
        "# Run the main function\n",
        "import asyncio\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljr1khCf3xL-",
        "outputId": "793bdb59-ddd8-4586-c3c7-43358479f870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trace started: trace_c8db5f792bc64b3998ca730338b846e7\n",
            "Span started: span_cf82ca08443d4910901e2ef8\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': None,\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:05.530483+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span started: span_b95f6d422274426da485ac0e\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_b95f6d422274426da485ac0e',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'span_data': {'input': None,\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': None,\n",
            "               'type': 'generation',\n",
            "               'usage': None},\n",
            " 'started_at': '2025-06-26T06:14:05.531771+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_b95f6d422274426da485ac0e\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:06.809787+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_b95f6d422274426da485ac0e',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'span_data': {'input': [{'content': 'Perform example tasks.',\n",
            "                          'role': 'system'},\n",
            "                         {'content': 'Start the task', 'role': 'user'}],\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': [{'annotations': None,\n",
            "                           'audio': None,\n",
            "                           'content': \"Okay, I'm ready to start.  Please tell \"\n",
            "                                      \"me what you'd like me to do. The more \"\n",
            "                                      'specific you are, the better I can '\n",
            "                                      'perform the task!  For example, you '\n",
            "                                      'could say:\\n'\n",
            "                                      '\\n'\n",
            "                                      '*   \"Write a short poem about a cat.\"\\n'\n",
            "                                      '*   \"Summarize the main points of the '\n",
            "                                      'article about the decline of '\n",
            "                                      'honeybees.\"\\n'\n",
            "                                      '*   \"Translate \\'Hello, how are you?\\' '\n",
            "                                      'into Spanish.\"\\n'\n",
            "                                      '*   \"Find me 5 websites with '\n",
            "                                      'information about quantum physics.\"\\n'\n",
            "                                      '*   \"Calculate 15% of 250.\"\\n'\n",
            "                                      '\\n'\n",
            "                                      \"Let me know what you've got!\\n\",\n",
            "                           'function_call': None,\n",
            "                           'refusal': None,\n",
            "                           'role': 'assistant',\n",
            "                           'tool_calls': None}],\n",
            "               'type': 'generation',\n",
            "               'usage': {'input_tokens': 7, 'output_tokens': 131}},\n",
            " 'started_at': '2025-06-26T06:14:05.531771+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_cf82ca08443d4910901e2ef8\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:06.812260+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': [],\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:05.530483+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span started: span_0f18d622e11a49409e19c4ce\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': None,\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:06.812918+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span started: span_5e684db172554256ab4f678f\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_5e684db172554256ab4f678f',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'span_data': {'input': None,\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': None,\n",
            "               'type': 'generation',\n",
            "               'usage': None},\n",
            " 'started_at': '2025-06-26T06:14:06.813881+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_5e684db172554256ab4f678f\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:08.184326+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_5e684db172554256ab4f678f',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'span_data': {'input': [{'content': 'Perform example tasks.',\n",
            "                          'role': 'system'},\n",
            "                         {'content': \"Rate this result: Okay, I'm ready to \"\n",
            "                                     \"start.  Please tell me what you'd like \"\n",
            "                                     'me to do. The more specific you are, the '\n",
            "                                     'better I can perform the task!  For '\n",
            "                                     'example, you could say:\\n'\n",
            "                                     '\\n'\n",
            "                                     '*   \"Write a short poem about a cat.\"\\n'\n",
            "                                     '*   \"Summarize the main points of the '\n",
            "                                     'article about the decline of '\n",
            "                                     'honeybees.\"\\n'\n",
            "                                     '*   \"Translate \\'Hello, how are you?\\' '\n",
            "                                     'into Spanish.\"\\n'\n",
            "                                     '*   \"Find me 5 websites with information '\n",
            "                                     'about quantum physics.\"\\n'\n",
            "                                     '*   \"Calculate 15% of 250.\"\\n'\n",
            "                                     '\\n'\n",
            "                                     \"Let me know what you've got!\\n\",\n",
            "                          'role': 'user'}],\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': [{'annotations': None,\n",
            "                           'audio': None,\n",
            "                           'content': \"This is a very good response. Here's \"\n",
            "                                      'why:\\n'\n",
            "                                      '\\n'\n",
            "                                      '*   **Clear and Direct:** It '\n",
            "                                      \"immediately acknowledges the user's \"\n",
            "                                      'previous prompt (presumably the user '\n",
            "                                      'asked the AI if it was ready).\\n'\n",
            "                                      '*   **Proactive:** It encourages the '\n",
            "                                      'user to provide a task.\\n'\n",
            "                                      '*   **Specific Guidance:**  It '\n",
            "                                      'explicitly requests a specific task and '\n",
            "                                      'emphasizes the importance of detail for '\n",
            "                                      'better performance.\\n'\n",
            "                                      '*   **Helpful Examples:** The examples '\n",
            "                                      'provided are varied and cover different '\n",
            "                                      'types of tasks the AI might be capable '\n",
            "                                      'of handling. This gives the user a '\n",
            "                                      'clear idea of what they can ask for.\\n'\n",
            "                                      '*   **Friendly Tone:** The overall tone '\n",
            "                                      'is helpful and encouraging, making the '\n",
            "                                      'user feel comfortable providing a '\n",
            "                                      'request.\\n'\n",
            "                                      '\\n'\n",
            "                                      '**Overall Rating: 5/5**\\n',\n",
            "                           'function_call': None,\n",
            "                           'refusal': None,\n",
            "                           'role': 'assistant',\n",
            "                           'tool_calls': None}],\n",
            "               'type': 'generation',\n",
            "               'usage': {'input_tokens': 139, 'output_tokens': 156}},\n",
            " 'started_at': '2025-06-26T06:14:06.813881+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_0f18d622e11a49409e19c4ce\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:08.186528+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': [],\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:06.812918+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Result: Okay, I'm ready to start.  Please tell me what you'd like me to do. The more specific you are, the better I can perform the task!  For example, you could say:\n",
            "\n",
            "*   \"Write a short poem about a cat.\"\n",
            "*   \"Summarize the main points of the article about the decline of honeybees.\"\n",
            "*   \"Translate 'Hello, how are you?' into Spanish.\"\n",
            "*   \"Find me 5 websites with information about quantum physics.\"\n",
            "*   \"Calculate 15% of 250.\"\n",
            "\n",
            "Let me know what you've got!\n",
            "\n",
            "Rating: This is a very good response. Here's why:\n",
            "\n",
            "*   **Clear and Direct:** It immediately acknowledges the user's previous prompt (presumably the user asked the AI if it was ready).\n",
            "*   **Proactive:** It encourages the user to provide a task.\n",
            "*   **Specific Guidance:**  It explicitly requests a specific task and emphasizes the importance of detail for better performance.\n",
            "*   **Helpful Examples:** The examples provided are varied and cover different types of tasks the AI might be capable of handling. This gives the user a clear idea of what they can ask for.\n",
            "*   **Friendly Tone:** The overall tone is helpful and encouraging, making the user feel comfortable providing a request.\n",
            "\n",
            "**Overall Rating: 5/5**\n",
            "\n",
            "Trace ended: {'object': 'trace', 'id': 'trace_c8db5f792bc64b3998ca730338b846e7', 'workflow_name': 'Example workflow', 'group_id': None, 'metadata': None}\n"
          ]
        }
      ]
    }
  ]
}