{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnxc4k3jT9v4JPWb+j7vok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubaahmedkhan/AgenticAI-Toolkit-/blob/main/Tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_f2bN_NVkgw",
        "outputId": "4581f51b-e863-436e-fe5e-020815886477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/130.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "cgUDBIzxWAbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, ModelSettings, function_tool\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "ucW3rKjUWG2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "# Check if the API key is present; if not, raise an error\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "model_settings = ModelSettings(\n",
        "    max_tokens=50,\n",
        "                                              # CHECK All PARAMETERS PICTURE IN MOBILE\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    model_settings=model_settings,\n",
        "    tracing_disabled=True\n",
        ")"
      ],
      "metadata": {
        "id": "CtvFu_U3WUb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"helpful assistant\",\n",
        "    instructions=\"You are a helpful assistant.\",\n",
        ")\n",
        "\n",
        "result = await Runner.run(agent, \"hello, how are you\", run_config=config)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk7YaPwKWpg3",
        "outputId": "a026d367-4331-4a9a-f267-f6c9ea3b45ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResult:\n",
            "- Last agent: Agent(name=\"helpful assistant\", ...)\n",
            "- Final output (str):\n",
            "    Hello! As an AI, I don't experience feelings in the same way humans do. But I'm functioning well and ready to assist you. How can I help you today?\n",
            "- 1 new item(s)\n",
            "- 1 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResult` for more details)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.last_agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUG1uTj8YIm6",
        "outputId": "d6b2ae63-e236-4d36-a362-a52df2524278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Agent(name='helpful assistant', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.output_guardrail_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToQZSTcIYlfq",
        "outputId": "b51e1080-9f75-4512-e3ea-fd7c0e9bdc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.new_items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0BOJuDbY1Zx",
        "outputId": "41c16342-7179-4d13-ebc7-0feee4ad1686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[MessageOutputItem(agent=Agent(name='helpful assistant', instructions='You are a helpful assistant.', prompt=None, handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Hello! How can I help you today?\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.raw_responses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z2I3m23ZC4Y",
        "outputId": "954fe2ac-4999-473f-a788-98825d099335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Hello! As an AI, I don't experience feelings in the same way humans do, but I am functioning well and ready to assist you. How can I help you today?\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=11, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=38, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=49), response_id=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üí° Trace aur Span ka matlab kya hai?**\n",
        "Jab aap kisi AI workflow ko chalatay hain ‚Äî jaise ke ek agent user se baat karta hai, tool call karta hai, LLM se jawab mangta hai ‚Äî to poora process ko ek Trace kehtay hain.\n",
        "\n",
        "Aur Trace ke andar chhoti chhoti activities ko Spans kehtay hain.\n",
        "\n",
        "üü¶ Trace ‚Äì Pura Workflow ka Record\n",
        "Trace represent karta hai ek poora end-to-end kaam ‚Äî jaise:\n",
        "\n",
        "\"Customer Support Agent ne user ka masla hal kia\"\n",
        "\n",
        "Trace ke Properties:\n",
        "workflow_name ‚Äì Workflow ka naam, jaise \"Code generation\" ya \"Customer support\".\n",
        "\n",
        "trace_id ‚Äì Unique ID hoti hai har trace ki. Format: trace_<32 letters/numbers>. (Agar na dein to system khud bana deta hai.)\n",
        "\n",
        "group_id ‚Äì Optional hai. Agar aap multiple traces ko ek group mein rakhna chahtay hain (jaise ek hi user ka chat session), to yeh use hota hai.\n",
        "\n",
        "disabled ‚Äì Agar True ho, to trace record nahi hoti.\n",
        "\n",
        "metadata ‚Äì Aap kuch extra information bhi trace ke sath attach kar saktay hain (jaise user ID, country etc.)\n",
        "\n",
        "\n",
        "üü® Span ‚Äì Trace ke Andar Ek Chhota Hissa\n",
        "Har trace ke andar chhoti chhoti steps hoti hain, jinko Spans kehtay hain.\n",
        "\n",
        "Jaise:\n",
        "\n",
        "Agent call karna\n",
        "\n",
        "LLM se response lena\n",
        "\n",
        "Tool run karna\n",
        "\n",
        "Data store karna\n",
        "\n",
        "Span ke Properties:\n",
        "started_at & ended_at ‚Äì Start aur end ka time.\n",
        "\n",
        "trace_id ‚Äì Ye span kis trace se related hai.\n",
        "\n",
        "parent_id ‚Äì Agar yeh span kisi aur span ke andar hua, to uska ID.\n",
        "\n",
        "span_data ‚Äì Yeh batata hai span kis kaam ke liye tha:\n",
        "\n",
        "AgentSpanData ‚Äì agar agent chal raha tha\n",
        "\n",
        "GenerationSpanData ‚Äì agar LLM se jawab aaya tha\n",
        "\n",
        "ToolSpanData ‚Äì agar koi tool run hua tha\n",
        "\n",
        "\n",
        "**‚úÖ Default Tracing ka Matlab**\n",
        "Jab aap OpenAI Agent SDK ka use karte hain, to SDK khud ba khud (automatically) kuch important cheezon ka trace record karta hai ‚Äî aapko manually kuch likhne ki zarurat nahi hoti.\n",
        "\n",
        "\n",
        "| Kaam                                               | Span Type              | Kya Trace Hota Hai?                    |\n",
        "| -------------------------------------------------- | ---------------------- | -------------------------------------- |\n",
        "| `Runner.run()` ya `run_sync()` ya `run_streamed()` | `trace()`              | Poora execution                        |\n",
        "| Agent ka chalna                                    | `agent_span()`         | Jab agent kaam kar raha ho             |\n",
        "| LLM ka jawab dena                                  | `generation_span()`    | LLM se prompt bhejna aur response lena |\n",
        "| Tool ka call hona                                  | `function_span()`      | Jab koi tool use ho                    |\n",
        "| Guardrail ka kaam                                  | `guardrail_span()`     | Agar guardrails lage ho                |\n",
        "| Handoff (ek agent se doosray ko kaam dena)         | `handoff_span()`       | Jab agent doosray ko handoff kare      |\n",
        "| Audio input (Speech to Text)                       | `transcription_span()` | User ki awaaz ko text mein badalna     |\n",
        "| Audio output (Text to Speech)                      | `speech_span()`        | Text ko awaaz mein badalna             |\n",
        "| Related audio spans grouping                       | `speech_group_span()`  | Sab audio spans ko group karna         |\n",
        "\n",
        "\n",
        "\n",
        "üè∑Ô∏è **Default Trace Name**\n",
        "SDK trace ka default naam hota hai: \"Agent trace\"\n",
        "\n",
        "Lekin agar aap chahein to trace() ya RunConfig mein naam aur properties customize kar saktay hain.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T5RCUJJ4bABe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from agents import Runner,RunConfig\n",
        "\n",
        "# Custom RunConfig\n",
        "run_config = RunConfig(\n",
        "    trace_name=\"Customer Support Workflow\",  # custom trace name\n",
        "    trace_metadata={\"user_id\": \"user_123\", \"session\": \"chat_456\"},\n",
        "    trace_group_id=\"conversation_group_1\"\n",
        ")\n",
        "\n",
        "# Runner with the custom config\n",
        "result = await Runner.run(agent=agent, input=\"Hello!\", run_config=run_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "USMQIQOsegNE",
        "outputId": "1882e26e-490a-4294-cfb6-91a12cfcf4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "RunConfig.__init__() got an unexpected keyword argument 'trace_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-2023049607.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Custom RunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m run_config = RunConfig(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrace_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Customer Support Workflow\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# custom trace name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrace_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user_123\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"session\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"chat_456\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: RunConfig.__init__() got an unexpected keyword argument 'trace_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Higher level traces**\n",
        "Sometimes, you might want multiple calls to run() to be part of a single trace. You can do this by wrapping the entire code in a trace()."
      ],
      "metadata": {
        "id": "Ix7RL_3dhIwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, trace\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n",
        "\n",
        "    with trace(\"Joke workflow\"):\n",
        "        first_result = await Runner.run(agent, \"Tell me a joke\",run_config=config)\n",
        "        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\",run_config=config)\n",
        "        print(f\"Joke: {first_result.final_output}\")\n",
        "        print(f\"Rating: {second_result.final_output}\")\n",
        "\n",
        "# For running the async main\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85x4m5dugSMZ",
        "outputId": "6ecd5980-93e9-42bd-c631-489b52db960d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joke: Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "Rating: Okay, that's a solid classic! I'd give it a **7/10**. It's punny, easily understandable, and gets a chuckle. It's not going to win any comedy awards, but it's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trace Locally**"
      ],
      "metadata": {
        "id": "VTHc3IU04-t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import AsyncOpenAI\n",
        "from agents import Agent, Runner, trace, set_default_openai_api, set_default_openai_client, set_trace_processors\n",
        "from agents.tracing.processor_interface import TracingProcessor\n",
        "from pprint import pprint\n",
        "# Custom trace processor to collect trace data locally\n",
        "class LocalTraceProcessor(TracingProcessor):\n",
        "    def __init__(self):\n",
        "        self.traces = []\n",
        "        self.spans = []\n",
        "\n",
        "    def on_trace_start(self, trace):\n",
        "        self.traces.append(trace)\n",
        "        print(f\"Trace started: {trace.trace_id}\")\n",
        "\n",
        "    def on_trace_end(self, trace):\n",
        "        print(f\"Trace ended: {trace.export()}\")\n",
        "\n",
        "    def on_span_start(self, span):\n",
        "        self.spans.append(span)\n",
        "        print(f\"Span started: {span.span_id}\")\n",
        "        print(f\"Span details: \")\n",
        "        pprint(span.export())\n",
        "\n",
        "    def on_span_end(self, span):\n",
        "        print(f\"Span ended: {span.span_id}\")\n",
        "        print(f\"Span details:\")\n",
        "        pprint(span.export())\n",
        "\n",
        "    def force_flush(self):\n",
        "        print(\"Forcing flush of trace data\")\n",
        "\n",
        "    def shutdown(self):\n",
        "        print(\"=======Shutting down trace processor========\")\n",
        "        # Print all collected trace and span data\n",
        "        print(\"Collected Traces:\")\n",
        "        for trace in self.traces:\n",
        "            print(trace.export())\n",
        "        print(\"Collected Spans:\")\n",
        "        for span in self.spans:\n",
        "            print(span.export())\n",
        "\n",
        "BASE_URL = os.getenv(\"BASE_URL\") or \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\") or userdata.get(\"GEMINI_API_KEY\")\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\") or \"gemini-2.0-flash\"\n",
        "\n",
        "\n",
        "if not BASE_URL or not API_KEY or not MODEL_NAME:\n",
        "    raise ValueError(\"Please set BASE_URL, GEMINI_API_KEY, MODEL_NAME via env var or code.\")\n",
        "\n",
        "# Create OpenAI client\n",
        "client = AsyncOpenAI(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=API_KEY,\n",
        ")\n",
        "\n",
        "# Configure the client\n",
        "set_default_openai_client(client=client, use_for_tracing=True)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "\n",
        "# Set up the custom trace processor\n",
        "local_processor = LocalTraceProcessor()\n",
        "set_trace_processors([local_processor])\n",
        "\n",
        "# Example function to run an agent and collect traces\n",
        "async def main():\n",
        "    agent = Agent(name=\"Example Agent\", instructions=\"Perform example tasks.\", model=MODEL_NAME)\n",
        "\n",
        "    with trace(\"Example workflow\"):\n",
        "        first_result = await Runner.run(agent, \"Start the task\")\n",
        "        second_result = await Runner.run(agent, f\"Rate this result: {first_result.final_output}\")\n",
        "        print(f\"Result: {first_result.final_output}\")\n",
        "        print(f\"Rating: {second_result.final_output}\")\n",
        "\n",
        "# Run the main function\n",
        "import asyncio\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljr1khCf3xL-",
        "outputId": "793bdb59-ddd8-4586-c3c7-43358479f870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trace started: trace_c8db5f792bc64b3998ca730338b846e7\n",
            "Span started: span_cf82ca08443d4910901e2ef8\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': None,\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:05.530483+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span started: span_b95f6d422274426da485ac0e\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_b95f6d422274426da485ac0e',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'span_data': {'input': None,\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': None,\n",
            "               'type': 'generation',\n",
            "               'usage': None},\n",
            " 'started_at': '2025-06-26T06:14:05.531771+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_b95f6d422274426da485ac0e\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:06.809787+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_b95f6d422274426da485ac0e',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'span_data': {'input': [{'content': 'Perform example tasks.',\n",
            "                          'role': 'system'},\n",
            "                         {'content': 'Start the task', 'role': 'user'}],\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': [{'annotations': None,\n",
            "                           'audio': None,\n",
            "                           'content': \"Okay, I'm ready to start.  Please tell \"\n",
            "                                      \"me what you'd like me to do. The more \"\n",
            "                                      'specific you are, the better I can '\n",
            "                                      'perform the task!  For example, you '\n",
            "                                      'could say:\\n'\n",
            "                                      '\\n'\n",
            "                                      '*   \"Write a short poem about a cat.\"\\n'\n",
            "                                      '*   \"Summarize the main points of the '\n",
            "                                      'article about the decline of '\n",
            "                                      'honeybees.\"\\n'\n",
            "                                      '*   \"Translate \\'Hello, how are you?\\' '\n",
            "                                      'into Spanish.\"\\n'\n",
            "                                      '*   \"Find me 5 websites with '\n",
            "                                      'information about quantum physics.\"\\n'\n",
            "                                      '*   \"Calculate 15% of 250.\"\\n'\n",
            "                                      '\\n'\n",
            "                                      \"Let me know what you've got!\\n\",\n",
            "                           'function_call': None,\n",
            "                           'refusal': None,\n",
            "                           'role': 'assistant',\n",
            "                           'tool_calls': None}],\n",
            "               'type': 'generation',\n",
            "               'usage': {'input_tokens': 7, 'output_tokens': 131}},\n",
            " 'started_at': '2025-06-26T06:14:05.531771+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_cf82ca08443d4910901e2ef8\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:06.812260+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_cf82ca08443d4910901e2ef8',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': [],\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:05.530483+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span started: span_0f18d622e11a49409e19c4ce\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': None,\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:06.812918+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span started: span_5e684db172554256ab4f678f\n",
            "Span details: \n",
            "{'ended_at': None,\n",
            " 'error': None,\n",
            " 'id': 'span_5e684db172554256ab4f678f',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'span_data': {'input': None,\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': None,\n",
            "               'type': 'generation',\n",
            "               'usage': None},\n",
            " 'started_at': '2025-06-26T06:14:06.813881+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_5e684db172554256ab4f678f\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:08.184326+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_5e684db172554256ab4f678f',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'span_data': {'input': [{'content': 'Perform example tasks.',\n",
            "                          'role': 'system'},\n",
            "                         {'content': \"Rate this result: Okay, I'm ready to \"\n",
            "                                     \"start.  Please tell me what you'd like \"\n",
            "                                     'me to do. The more specific you are, the '\n",
            "                                     'better I can perform the task!  For '\n",
            "                                     'example, you could say:\\n'\n",
            "                                     '\\n'\n",
            "                                     '*   \"Write a short poem about a cat.\"\\n'\n",
            "                                     '*   \"Summarize the main points of the '\n",
            "                                     'article about the decline of '\n",
            "                                     'honeybees.\"\\n'\n",
            "                                     '*   \"Translate \\'Hello, how are you?\\' '\n",
            "                                     'into Spanish.\"\\n'\n",
            "                                     '*   \"Find me 5 websites with information '\n",
            "                                     'about quantum physics.\"\\n'\n",
            "                                     '*   \"Calculate 15% of 250.\"\\n'\n",
            "                                     '\\n'\n",
            "                                     \"Let me know what you've got!\\n\",\n",
            "                          'role': 'user'}],\n",
            "               'model': 'gemini-2.0-flash',\n",
            "               'model_config': {'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai/',\n",
            "                                'extra_args': None,\n",
            "                                'extra_body': None,\n",
            "                                'extra_headers': None,\n",
            "                                'extra_query': None,\n",
            "                                'frequency_penalty': None,\n",
            "                                'include_usage': None,\n",
            "                                'max_tokens': None,\n",
            "                                'metadata': None,\n",
            "                                'parallel_tool_calls': None,\n",
            "                                'presence_penalty': None,\n",
            "                                'reasoning': None,\n",
            "                                'store': None,\n",
            "                                'temperature': None,\n",
            "                                'tool_choice': None,\n",
            "                                'top_p': None,\n",
            "                                'truncation': None},\n",
            "               'output': [{'annotations': None,\n",
            "                           'audio': None,\n",
            "                           'content': \"This is a very good response. Here's \"\n",
            "                                      'why:\\n'\n",
            "                                      '\\n'\n",
            "                                      '*   **Clear and Direct:** It '\n",
            "                                      \"immediately acknowledges the user's \"\n",
            "                                      'previous prompt (presumably the user '\n",
            "                                      'asked the AI if it was ready).\\n'\n",
            "                                      '*   **Proactive:** It encourages the '\n",
            "                                      'user to provide a task.\\n'\n",
            "                                      '*   **Specific Guidance:**  It '\n",
            "                                      'explicitly requests a specific task and '\n",
            "                                      'emphasizes the importance of detail for '\n",
            "                                      'better performance.\\n'\n",
            "                                      '*   **Helpful Examples:** The examples '\n",
            "                                      'provided are varied and cover different '\n",
            "                                      'types of tasks the AI might be capable '\n",
            "                                      'of handling. This gives the user a '\n",
            "                                      'clear idea of what they can ask for.\\n'\n",
            "                                      '*   **Friendly Tone:** The overall tone '\n",
            "                                      'is helpful and encouraging, making the '\n",
            "                                      'user feel comfortable providing a '\n",
            "                                      'request.\\n'\n",
            "                                      '\\n'\n",
            "                                      '**Overall Rating: 5/5**\\n',\n",
            "                           'function_call': None,\n",
            "                           'refusal': None,\n",
            "                           'role': 'assistant',\n",
            "                           'tool_calls': None}],\n",
            "               'type': 'generation',\n",
            "               'usage': {'input_tokens': 139, 'output_tokens': 156}},\n",
            " 'started_at': '2025-06-26T06:14:06.813881+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Span ended: span_0f18d622e11a49409e19c4ce\n",
            "Span details:\n",
            "{'ended_at': '2025-06-26T06:14:08.186528+00:00',\n",
            " 'error': None,\n",
            " 'id': 'span_0f18d622e11a49409e19c4ce',\n",
            " 'object': 'trace.span',\n",
            " 'parent_id': None,\n",
            " 'span_data': {'handoffs': [],\n",
            "               'name': 'Example Agent',\n",
            "               'output_type': 'str',\n",
            "               'tools': [],\n",
            "               'type': 'agent'},\n",
            " 'started_at': '2025-06-26T06:14:06.812918+00:00',\n",
            " 'trace_id': 'trace_c8db5f792bc64b3998ca730338b846e7'}\n",
            "Result: Okay, I'm ready to start.  Please tell me what you'd like me to do. The more specific you are, the better I can perform the task!  For example, you could say:\n",
            "\n",
            "*   \"Write a short poem about a cat.\"\n",
            "*   \"Summarize the main points of the article about the decline of honeybees.\"\n",
            "*   \"Translate 'Hello, how are you?' into Spanish.\"\n",
            "*   \"Find me 5 websites with information about quantum physics.\"\n",
            "*   \"Calculate 15% of 250.\"\n",
            "\n",
            "Let me know what you've got!\n",
            "\n",
            "Rating: This is a very good response. Here's why:\n",
            "\n",
            "*   **Clear and Direct:** It immediately acknowledges the user's previous prompt (presumably the user asked the AI if it was ready).\n",
            "*   **Proactive:** It encourages the user to provide a task.\n",
            "*   **Specific Guidance:**  It explicitly requests a specific task and emphasizes the importance of detail for better performance.\n",
            "*   **Helpful Examples:** The examples provided are varied and cover different types of tasks the AI might be capable of handling. This gives the user a clear idea of what they can ask for.\n",
            "*   **Friendly Tone:** The overall tone is helpful and encouraging, making the user feel comfortable providing a request.\n",
            "\n",
            "**Overall Rating: 5/5**\n",
            "\n",
            "Trace ended: {'object': 'trace', 'id': 'trace_c8db5f792bc64b3998ca730338b846e7', 'workflow_name': 'Example workflow', 'group_id': None, 'metadata': None}\n"
          ]
        }
      ]
    }
  ]
}